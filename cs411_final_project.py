# -*- coding: utf-8 -*-
"""CS411_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_WZkRcOBBbA-a-647YaXnX15FMJQXo2
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import hashlib
import os
import shutil
import time
import matplotlib.pyplot as plt

# Option 1: Load from Kaggle (requires kagglehub)
def load_from_kaggle():
    """Load the Olist dataset from Kaggle"""
    try:
        import kagglehub

        # Download the dataset
        path = kagglehub.dataset_download("terencicp/e-commerce-dataset-by-olist-as-an-sqlite-database")
        print(f"Dataset downloaded to: {path}")

        # List files in the downloaded directory to help locate the database file
        print("Files in downloaded directory:")
        for root, dirs, files in os.walk(path):
            for name in files:
                print(os.path.join(root, name))
            for name in dirs:
                print(os.path.join(root, name))


        # Define the source and destination paths
        # Corrected path based on the file listing in the previous execution
        source_db_path = os.path.join(path, "olist.sqlite")
        destination_db_path = "/content/ecommerce.db" # Keep the destination name consistent

        # Copy the database file to a writable location
        shutil.copyfile(source_db_path, destination_db_path)
        print(f"Database copied to: {destination_db_path}")


        # Connect to the SQLite database
        source_db = sqlite3.connect(destination_db_path)

        # Load all tables into pandas DataFrames
        tables = {}
        cursor = source_db.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        table_names = cursor.fetchall()

        for table in table_names:
            table_name = table[0]
            tables[table_name] = pd.read_sql_query(f"SELECT * FROM {table_name}", source_db)
            print(f"Loaded {table_name}: {len(tables[table_name])} rows")

        source_db.close()
        return tables
    except ImportError:
        print("kagglehub not installed. Using alternative method...")
        return None

def create_database_schema(conn):
    """Create the optimized database schema with proper indexes"""
    cursor = conn.cursor()

    # Drop existing tables if they exist
    tables_to_drop = ['Reviews', 'Payments', 'Order_Items', 'Orders', 'Inventory', 'Customers', 'Products']
    for table in tables_to_drop:
        cursor.execute(f"DROP TABLE IF EXISTS {table}")

    # Create Products table
    cursor.execute("""
    CREATE TABLE Products (
        product_id VARCHAR(255) PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        description TEXT,
        weight_g DECIMAL(10,2),
        length_cm DECIMAL(10,2),
        height_cm DECIMAL(10,2),
        width_cm DECIMAL(10,2),
        category_name VARCHAR(100),
        photos_qty INTEGER DEFAULT 0
    )
    """)

    # Create Inventory table
    cursor.execute("""
    CREATE TABLE Inventory (
        inventory_id INTEGER PRIMARY KEY AUTOINCREMENT,
        product_id VARCHAR(255) UNIQUE NOT NULL,
        available_qty INTEGER DEFAULT 0,
        reserved_qty INTEGER DEFAULT 0,
        restock_date DATE,
        FOREIGN KEY (product_id) REFERENCES Products(product_id) ON DELETE CASCADE
    )
    """)

    # Create Customers table
    cursor.execute("""
    CREATE TABLE Customers (
        customer_id VARCHAR(255) PRIMARY KEY,
        customer_unique_id VARCHAR(255),
        name VARCHAR(255) NOT NULL,
        email VARCHAR(255),
        phone VARCHAR(20),
        zip_code VARCHAR(10),
        city VARCHAR(100),
        state VARCHAR(100)
    )
    """)

    # Create Orders table
    cursor.execute("""
    CREATE TABLE Orders (
        order_id VARCHAR(255) PRIMARY KEY,
        customer_id VARCHAR(255) NOT NULL,
        seller_id VARCHAR(255),
        status VARCHAR(50) NOT NULL DEFAULT 'pending',
        purchase_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        approved_at TIMESTAMP,
        delivered_carrier_date TIMESTAMP,
        delivered_customer_date TIMESTAMP,
        est_delivery_date DATE,
        FOREIGN KEY (customer_id) REFERENCES Customers(customer_id) ON DELETE RESTRICT
    )
    """)

    # Create Order_Items table
    cursor.execute("""
    CREATE TABLE Order_Items (
        order_item_id INTEGER PRIMARY KEY AUTOINCREMENT,
        order_id VARCHAR(255) NOT NULL,
        product_id VARCHAR(255) NOT NULL,
        seller_id VARCHAR(255),
        quantity INTEGER NOT NULL,
        unit_price DECIMAL(10,2) NOT NULL,
        freight_value DECIMAL(10,2) DEFAULT 0,
        shipping_limit_date TIMESTAMP,
        FOREIGN KEY (order_id) REFERENCES Orders(order_id) ON DELETE CASCADE,
        FOREIGN KEY (product_id) REFERENCES Products(product_id) ON DELETE RESTRICT
    )
    """)

    # Create Payments table
    cursor.execute("""
    CREATE TABLE Payments (
        payment_id INTEGER PRIMARY KEY AUTOINCREMENT,
        order_id VARCHAR(255) NOT NULL,
        payment_sequential INTEGER DEFAULT 1,
        method VARCHAR(50) NOT NULL,
        installment_no INTEGER DEFAULT 1,
        total_installments INTEGER DEFAULT 1,
        amount DECIMAL(10,2) NOT NULL,
        FOREIGN KEY (order_id) REFERENCES Orders(order_id) ON DELETE CASCADE
    )
    """)

    # Create Reviews table
    cursor.execute("""
    CREATE TABLE Reviews (
        review_id VARCHAR(255) PRIMARY KEY,
        customer_id VARCHAR(255) NOT NULL,
        product_id VARCHAR(255) NOT NULL,
        order_id VARCHAR(255),
        score INTEGER NOT NULL CHECK (score >= 1 AND score <= 5),
        title VARCHAR(255),
        message TEXT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        answered_at TIMESTAMP,
        FOREIGN KEY (customer_id) REFERENCES Customers(customer_id) ON DELETE CASCADE,
        FOREIGN KEY (product_id) REFERENCES Products(product_id) ON DELETE CASCADE,
        FOREIGN KEY (order_id) REFERENCES Orders(order_id) ON DELETE SET NULL
    )
    """)

    conn.commit()
    print("✓ Database schema created successfully")

def create_indexes(conn):
    """Create optimized indexes for better query performance"""
    cursor = conn.cursor()

    index_statements = [
        # Products indexes
        "CREATE INDEX idx_products_category ON Products(category_name)",
        "CREATE INDEX idx_products_title ON Products(title)",

        # Inventory indexes
        "CREATE INDEX idx_inventory_product ON Inventory(product_id)",
        "CREATE INDEX idx_inventory_available ON Inventory(available_qty)",

        # Customers indexes
        "CREATE INDEX idx_customers_email ON Customers(email)",
        "CREATE INDEX idx_customers_location ON Customers(city, state)",
        "CREATE INDEX idx_customers_unique ON Customers(customer_unique_id)",

        # Orders indexes
        "CREATE INDEX idx_orders_customer ON Orders(customer_id)",
        "CREATE INDEX idx_orders_status ON Orders(status)",
        "CREATE INDEX idx_orders_purchase ON Orders(purchase_ts)",
        "CREATE INDEX idx_orders_seller ON Orders(seller_id)",
        "CREATE INDEX idx_orders_composite ON Orders(status, purchase_ts)",

        # Order_Items indexes
        "CREATE INDEX idx_order_items_order ON Order_Items(order_id)",
        "CREATE INDEX idx_order_items_product ON Order_Items(product_id)",
        "CREATE INDEX idx_order_items_seller ON Order_Items(seller_id)",
        "CREATE INDEX idx_order_items_composite ON Order_Items(order_id, product_id)",

        # Payments indexes
        "CREATE INDEX idx_payments_order ON Payments(order_id)",
        "CREATE INDEX idx_payments_method ON Payments(method)",

        # Reviews indexes
        "CREATE INDEX idx_reviews_product ON Reviews(product_id)",
        "CREATE INDEX idx_reviews_customer ON Reviews(customer_id)",
        "CREATE INDEX idx_reviews_order ON Reviews(order_id)",
        "CREATE INDEX idx_reviews_score ON Reviews(score)",
        "CREATE INDEX idx_reviews_created ON Reviews(created_at)"
    ]

    for index_stmt in index_statements:
        try:
            cursor.execute(index_stmt)
        except sqlite3.OperationalError as e:
            # Index might already exist
            print(f"Warning: {e}")

    conn.commit()
    print("✓ Indexes created successfully")

def populate_database(conn, olist_data):
    """Populate database with Olist dataset"""
    cursor = conn.cursor()

    # 1. Populate Products
    print("\nPopulating Products table...")
    if 'products' in olist_data: # Corrected key
        products_df = olist_data['products']

        # Add category information if available
        if 'product_category_name_translation' in olist_data: # Corrected key
            category_df = olist_data['product_category_name_translation']
            products_df = products_df.merge(
                category_df[['product_category_name', 'product_category_name_english']],
                left_on='product_category_name',
                right_on='product_category_name',
                how='left'
            )

        for _, row in products_df.iterrows():
            cursor.execute("""
                INSERT OR IGNORE INTO Products
                (product_id, title, description, weight_g, length_cm, height_cm, width_cm, category_name, photos_qty)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                row.get('product_id'),
                row.get('product_category_name_english', row.get('product_category_name', 'Unknown')),
                f"Product in category: {row.get('product_category_name_english', row.get('product_category_name', 'Unknown'))}",
                row.get('product_weight_g'),
                row.get('product_length_cm'),
                row.get('product_height_cm'),
                row.get('product_width_cm'),
                row.get('product_category_name_english', row.get('product_category_name')),
                row.get('product_photos_qty', 0)
            ))
        print(f"✓ Inserted {cursor.rowcount} products")
    else:
        print("Warning: 'products' not found in olist_data. Products table not populated.")


    # 2. Populate Inventory (generate random inventory data)
    print("\nPopulating Inventory table...")
    cursor.execute("SELECT product_id FROM Products")
    product_ids = cursor.fetchall()

    for product_id in product_ids:
        available = random.randint(0, 500)
        reserved = random.randint(0, min(50, available))
        restock_date = None
        if available < 20:
            restock_date = (datetime.now() + timedelta(days=random.randint(1, 30))).strftime('%Y-%m-%d')

        cursor.execute("""
            INSERT INTO Inventory (product_id, available_qty, reserved_qty, restock_date)
            VALUES (?, ?, ?, ?)
        """, (product_id[0], available, reserved, restock_date))
    print(f"✓ Created inventory records for {len(product_ids)} products")

    # 3. Populate Customers
    print("\nPopulating Customers table...")
    if 'customers' in olist_data: # Corrected key
        customers_df = olist_data['customers']

        # Generate names and emails for privacy
        for idx, row in customers_df.iterrows():
            # Generate a name based on customer_id hash
            name_hash = hashlib.md5(row['customer_id'].encode()).hexdigest()[:8]
            name = f"Customer_{name_hash}"
            email = f"customer_{name_hash}@example.com"
            phone = f"555-{random.randint(1000, 9999)}"

            cursor.execute("""
                INSERT OR IGNORE INTO Customers
                (customer_id, customer_unique_id, name, email, phone, zip_code, city, state)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                row.get('customer_id'),
                row.get('customer_unique_id'),
                name,
                email,
                phone,
                row.get('customer_zip_code_prefix'),
                row.get('customer_city'),
                row.get('customer_state')
            ))
        print(f"✓ Inserted {len(customers_df)} customers")
    else:
        print("Warning: 'customers' not found in olist_data. Customers table not populated.")


    # 4. Populate Orders
    print("\nPopulating Orders table...")
    if 'orders' in olist_data: # Corrected key
        orders_df = olist_data['orders']

        for _, row in orders_df.iterrows():
            cursor.execute("""
                INSERT OR IGNORE INTO Orders
                (order_id, customer_id, status, purchase_ts, approved_at,
                 delivered_carrier_date, delivered_customer_date, est_delivery_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                row.get('order_id'),
                row.get('customer_id'),
                row.get('order_status'),
                row.get('order_purchase_timestamp'),
                row.get('order_approved_at'),
                row.get('order_delivered_carrier_date'),
                row.get('order_delivered_customer_date'),
                row.get('order_estimated_delivery_date')
            ))
        print(f"✓ Inserted {len(orders_df)} orders")
    else:
        print("Warning: 'orders' not found in olist_data. Orders table not populated.")

    # 5. Populate Order_Items
    print("\nPopulating Order_Items table...")
    if 'order_items' in olist_data: # Corrected key
        order_items_df = olist_data['order_items']

        for _, row in order_items_df.iterrows():
            cursor.execute("""
                INSERT OR IGNORE INTO Order_Items
                (order_id, product_id, seller_id, quantity, unit_price, freight_value, shipping_limit_date)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                row.get('order_id'),
                row.get('product_id'),
                row.get('seller_id'),
                row.get('order_item_id', 1),  # Use order_item_id as quantity if available
                row.get('price'),
                row.get('freight_value'),
                row.get('shipping_limit_date')
            ))
        print(f"✓ Inserted {len(order_items_df)} order items")
    else:
        print("Warning: 'order_items' not found in olist_data. Order_Items table not populated.")


    # 6. Populate Payments
    print("\nPopulating Payments table...")
    if 'order_payments' in olist_data: # Corrected key
        payments_df = olist_data['order_payments']

        for _, row in payments_df.iterrows():
            cursor.execute("""
                INSERT OR IGNORE INTO Payments
                (order_id, payment_sequential, method, installment_no, total_installments, amount)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                row.get('order_id'),
                row.get('payment_sequential'),
                row.get('payment_type'),
                row.get('payment_sequential', 1),
                row.get('payment_installments'),
                row.get('payment_value')
            ))
        print(f"✓ Inserted {len(payments_df)} payment records")
    else:
        print("Warning: 'order_payments' not found in olist_data. Payments table not populated.")


    # 7. Populate Reviews
    print("\nPopulating Reviews table...")
    if 'order_reviews' in olist_data: # Corrected key
        reviews_df = olist_data['order_reviews']

        # Get order-customer-product mappings
        cursor.execute("""
            SELECT DISTINCT o.order_id, o.customer_id, oi.product_id
            FROM Orders o
            JOIN Order_Items oi ON o.order_id = oi.order_id
        """)
        order_mappings = {row[0]: (row[1], row[2]) for row in cursor.fetchall()}

        for _, row in reviews_df.iterrows():
            order_id = row.get('order_id')
            if order_id in order_mappings:
                customer_id, product_id = order_mappings[order_id]

                cursor.execute("""
                    INSERT OR IGNORE INTO Reviews
                    (review_id, customer_id, product_id, order_id, score, title, message, created_at, answered_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    row.get('review_id'),
                    customer_id,
                    product_id,
                    order_id,
                    row.get('review_score'),
                    row.get('review_comment_title'),
                    row.get('review_comment_message'),
                    row.get('review_creation_date'),
                    row.get('review_answer_timestamp')
                ))
        print(f"✓ Inserted reviews")
    else:
        print("Warning: 'order_reviews' not found in olist_data. Reviews table not populated.")


    conn.commit()
    print("\n✓ Database populated successfully!")

def run_advanced_queries(conn):
    """Run the 4 advanced SQL queries and display results"""
    cursor = conn.cursor()

    print("\n" + "="*80)
    print("RUNNING ADVANCED SQL QUERIES")
    print("="*80)

    # Query 1: Top Selling Products with Revenue Analysis
    print("\n1. TOP SELLING PRODUCTS WITH REVENUE ANALYSIS")
    print("-" * 60)

    query1 = """
    SELECT
        p.product_id,
        p.category_name,
        COUNT(DISTINCT o.order_id) as total_orders,
        SUM(oi.quantity) as total_quantity_sold,
        SUM(oi.quantity * oi.unit_price) as total_revenue,
        AVG(oi.unit_price) as avg_selling_price,
        MAX(o.purchase_ts) as last_sold_date
    FROM Products p
    INNER JOIN Order_Items oi ON p.product_id = oi.product_id
    INNER JOIN Orders o ON oi.order_id = o.order_id
    WHERE o.status IN ('delivered', 'shipped')

    GROUP BY p.product_id, p.category_name
    HAVING total_revenue > 1000
    ORDER BY total_revenue DESC
    LIMIT 15
    """

    cursor.execute(query1)
    results = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    df1 = pd.DataFrame(results, columns=columns)
    print(df1.to_string(index=False))

    # Query 2: Customer Lifetime Value with Review Engagement
    print("\n2. CUSTOMER LIFETIME VALUE WITH REVIEW ENGAGEMENT")
    print("-" * 60)

    query2 = """
    WITH CustomerMetrics AS (
        SELECT
            c.customer_id,
            c.name,
            c.city,
            c.state,
            COUNT(DISTINCT o.order_id) as order_count,
            SUM(p.amount) as total_spent,
            MIN(o.purchase_ts) as first_purchase,
            MAX(o.purchase_ts) as last_purchase
        FROM Customers c
        INNER JOIN Orders o ON c.customer_id = o.customer_id
        INNER JOIN Payments p ON o.order_id = p.order_id
        WHERE o.status != 'canceled'
        GROUP BY c.customer_id, c.name, c.city, c.state
    ),
    ReviewMetrics AS (
        SELECT
            customer_id,
            COUNT(*) as review_count,
            AVG(score) as avg_rating
        FROM Reviews
        GROUP BY customer_id
    )
    SELECT
        cm.customer_id,
        cm.name,
        cm.city,
        cm.state,
        cm.order_count,
        ROUND(cm.total_spent, 2) as total_spent,
        COALESCE(rm.review_count, 0) as reviews_written,
        ROUND(COALESCE(rm.avg_rating, 0), 2) as avg_rating_given,
        julianday(cm.last_purchase) - julianday(cm.first_purchase) as customer_lifetime_days,
        ROUND(cm.total_spent / cm.order_count, 2) as avg_order_value
    FROM CustomerMetrics cm
    LEFT JOIN ReviewMetrics rm ON cm.customer_id = rm.customer_id
    WHERE cm.total_spent > (
        SELECT AVG(total_spent) * 0.5
        FROM CustomerMetrics
    )
    ORDER BY cm.total_spent DESC
    LIMIT 15
    """

    cursor.execute(query2)
    results = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    df2 = pd.DataFrame(results, columns=columns)
    print(df2.to_string(index=False))

    # Query 3: Inventory Analysis with Sales Velocity
    print("\n3. INVENTORY ANALYSIS WITH SALES VELOCITY")
    print("-" * 60)

    query3 = """
    SELECT
        p.product_id,
        p.category_name,
        i.available_qty,
        i.reserved_qty,
        COALESCE(sales_data.units_sold_30d, 0) as units_sold_30d,
        COALESCE(sales_data.units_sold_7d, 0) as units_sold_7d,
        CASE
            WHEN COALESCE(sales_data.units_sold_7d, 0) > 0
            THEN ROUND(CAST(i.available_qty AS FLOAT) / (sales_data.units_sold_7d * 4.3), 2)
            ELSE 999
        END as weeks_of_inventory,
        COALESCE(pending.pending_orders, 0) as pending_order_count,
        i.restock_date
    FROM Products p
    INNER JOIN Inventory i ON p.product_id = i.product_id
    LEFT JOIN (
        SELECT
            oi.product_id,
            SUM(CASE WHEN o.purchase_ts >= date('now', '-30 days')
                THEN oi.quantity ELSE 0 END) as units_sold_30d,
            SUM(CASE WHEN o.purchase_ts >= date('now', '-7 days')
                THEN oi.quantity ELSE 0 END) as units_sold_7d
        FROM Order_Items oi
        INNER JOIN Orders o ON oi.order_id = o.order_id
        WHERE o.status IN ('delivered', 'shipped')
        GROUP BY oi.product_id
    ) sales_data ON p.product_id = sales_data.product_id
    LEFT JOIN (
        SELECT
            oi.product_id,
            COUNT(DISTINCT o.order_id) as pending_orders
        FROM Order_Items oi
        INNER JOIN Orders o ON oi.order_id = o.order_id
        WHERE o.status = 'processing'
        GROUP BY oi.product_id
    ) pending ON p.product_id = pending.product_id
    WHERE i.available_qty < 50
        OR (sales_data.units_sold_7d > 0 AND i.available_qty / (sales_data.units_sold_7d * 4.3) < 2)
    ORDER BY weeks_of_inventory ASC
    LIMIT 15
    """

    cursor.execute(query3)
    results = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    df3 = pd.DataFrame(results, columns=columns)
    print(df3.to_string(index=False))

    # Query 4: Payment Method Analysis with Order Performance
    print("\n4. PAYMENT METHOD ANALYSIS WITH ORDER PERFORMANCE")
    print("-" * 60)

    query4 = """
    WITH PaymentSummary AS (
        SELECT
            o.order_id,
            o.status,
            o.purchase_ts,
            GROUP_CONCAT(DISTINCT p.method) as payment_methods,
            COUNT(DISTINCT p.payment_id) as payment_count,
            SUM(p.amount) as total_paid,
            MAX(p.total_installments) as max_installments
        FROM Orders o
        INNER JOIN Payments p ON o.order_id = p.order_id
        GROUP BY o.order_id, o.status, o.purchase_ts
    ),
    OrderTotals AS (
        SELECT
            o.order_id,
            SUM(oi.quantity * oi.unit_price + oi.freight_value) as order_total
        FROM Orders o
        INNER JOIN Order_Items oi ON o.order_id = oi.order_id
        GROUP BY o.order_id
    )
    SELECT
        ps.payment_methods,
        COUNT(DISTINCT ps.order_id) as order_count,
        SUM(CASE WHEN ps.status = 'delivered' THEN 1 ELSE 0 END) as delivered_count,
        SUM(CASE WHEN ps.status = 'canceled' THEN 1 ELSE 0 END) as cancelled_count,
        ROUND(AVG(ot.order_total), 2) as avg_order_value,
        ROUND(AVG(ps.payment_count), 2) as avg_payment_splits,
        ROUND(AVG(ps.max_installments), 2) as avg_installments,
        ROUND(SUM(ps.total_paid), 2) as total_revenue,
        ROUND((SUM(CASE WHEN ps.status = 'delivered' THEN 1 ELSE 0 END) * 100.0 /
         COUNT(DISTINCT ps.order_id)), 2) as delivery_rate
    FROM PaymentSummary ps
    INNER JOIN OrderTotals ot ON ps.order_id = ot.order_id

    GROUP BY ps.payment_methods
    HAVING order_count > 10
    ORDER BY total_revenue DESC
    LIMIT 15
    """

    cursor.execute(query4)
    results = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    df4 = pd.DataFrame(results, columns=columns)
    print(df4.to_string(index=False))

    print("\n✓ All queries executed successfully!")

def analyze_query_performance(conn):
    """Analyze query performance with EXPLAIN QUERY PLAN"""
    cursor = conn.cursor()

    print("\n" + "="*80)
    print("QUERY PERFORMANCE ANALYSIS")
    print("="*80)

    queries = [
        ("Query 1 - Top Selling Products", """
            SELECT p.product_id, COUNT(DISTINCT o.order_id) as total_orders
            FROM Products p
            INNER JOIN Order_Items oi ON p.product_id = oi.product_id
            INNER JOIN Orders o ON oi.order_id = o.order_id
            WHERE o.status IN ('delivered', 'shipped')
            GROUP BY p.product_id
            ORDER BY total_orders DESC
            LIMIT 15
        """),
        ("Query 2 - Customer Lifetime Value", """
            SELECT c.customer_id, COUNT(DISTINCT o.order_id) as order_count
            FROM Customers c
            INNER JOIN Orders o ON c.customer_id = o.customer_id
            GROUP BY c.customer_id
            HAVING order_count > 5
            ORDER BY order_count DESC
            LIMIT 15
        """),
    ]

    for query_name, query in queries:
        print(f"\n{query_name}")
        print("-" * 40)

        # Run EXPLAIN QUERY PLAN
        cursor.execute(f"EXPLAIN QUERY PLAN {query}")
        plan = cursor.fetchall()

        for row in plan:
            print(f"  {row}")

        # Execute the query and measure time
        import time
        start_time = time.time()
        cursor.execute(query)
        results = cursor.fetchall()
        end_time = time.time()

        print(f"  Execution time: {(end_time - start_time)*1000:.2f} ms")
        print(f"  Rows returned: {len(results)}")

def get_database_statistics(conn):
    """Get database statistics"""
    cursor = conn.cursor()

    print("\n" + "="*80)
    print("DATABASE STATISTICS")
    print("="*80)

    tables = ['Products', 'Inventory', 'Customers', 'Orders', 'Order_Items', 'Payments', 'Reviews']

    for table in tables:
        cursor.execute(f"SELECT COUNT(*) FROM {table}")
        count = cursor.fetchone()[0]
        print(f"{table:20s}: {count:,} rows")

    # Get database size
    cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
    size = cursor.fetchone()[0]
    print(f"\nDatabase size: {size / (1024*1024):.2f} MB")

# ============================================================================
# PART 2: INDEXING ANALYSIS FUNCTIONS
# ============================================================================

def execute_with_timing(conn, query, query_name):
    """Execute query and return timing and EXPLAIN QUERY PLAN"""
    cursor = conn.cursor()

    # Get EXPLAIN QUERY PLAN
    cursor.execute(f"EXPLAIN QUERY PLAN {query}")
    plan = cursor.fetchall()

    # Execute and time the query (run 3 times and take median)
    times = []
    for _ in range(3):
        start_time = time.time()
        cursor.execute(query)
        results = cursor.fetchall()
        end_time = time.time()
        times.append((end_time - start_time) * 1000)  # Convert to milliseconds

    execution_time = np.median(times)

    return {
        'query_name': query_name,
        'execution_time_ms': execution_time,
        'plan': plan,
        'row_count': len(results)
    }

def analyze_query_with_indexes(conn, query, query_name, index_configs):
    """Test a query with different index configurations"""
    results = []

    print(f"\n{'='*80}")
    print(f"ANALYZING: {query_name}")
    print(f"{'='*80}")

    # Test baseline (no custom indexes - just primary keys)
    print("\n--- Configuration: BASELINE (Primary Keys Only) ---")
    result = execute_with_timing(conn, query, f"{query_name} - Baseline")
    print(f"Execution Time: {result['execution_time_ms']:.2f} ms")
    print(f"Rows Returned: {result['row_count']}")
    print("\nQuery Plan:")
    for row in result['plan']:
        print(f"  {row}")
    results.append(('Baseline', result['execution_time_ms']))

    # Test each index configuration
    for config_name, index_statements in index_configs:
        print(f"\n--- Configuration: {config_name} ---")

        # Create indexes
        cursor = conn.cursor()
        created_indexes = []
        for idx_stmt in index_statements:
            try:
                cursor.execute(idx_stmt)
                # Extract index name from statement
                idx_name = idx_stmt.split("CREATE INDEX ")[1].split(" ON ")[0]
                created_indexes.append(idx_name)
                print(f"Created: {idx_name}")
            except sqlite3.OperationalError as e:
                print(f"Warning: {e}")

        conn.commit()

        # Run query with these indexes
        result = execute_with_timing(conn, query, f"{query_name} - {config_name}")
        print(f"Execution Time: {result['execution_time_ms']:.2f} ms")
        print(f"Rows Returned: {result['row_count']}")
        print("\nQuery Plan:")
        for row in result['plan']:
            print(f"  {row}")

        results.append((config_name, result['execution_time_ms']))

        # Drop created indexes for next test
        for idx_name in created_indexes:
            try:
                cursor.execute(f"DROP INDEX IF EXISTS {idx_name}")
            except:
                pass
        conn.commit()

    return results

def plot_indexing_results(all_results, query_names):
    """Create visualization of indexing performance"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()

    for idx, (query_name, results) in enumerate(zip(query_names, all_results)):
        configs = [r[0] for r in results]
        times = [r[1] for r in results]

        ax = axes[idx]
        bars = ax.bar(range(len(configs)), times, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])
        ax.set_xlabel('Index Configuration', fontsize=10)
        ax.set_ylabel('Execution Time (ms)', fontsize=10)
        ax.set_title(f'Query {idx+1}: {query_name}', fontsize=12, fontweight='bold')
        ax.set_xticks(range(len(configs)))
        ax.set_xticklabels(configs, rotation=45, ha='right', fontsize=8)
        ax.grid(axis='y', alpha=0.3)

        # Add value labels on bars
        for i, (bar, time_val) in enumerate(zip(bars, times)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{time_val:.2f}ms',
                   ha='center', va='bottom', fontsize=8)

        # Add improvement percentage
        if len(times) > 1:
            baseline = times[0]
            best_time = min(times[1:])
            improvement = ((baseline - best_time) / baseline) * 100
            ax.text(0.02, 0.98, f'Best Improvement: {improvement:.1f}%',
                   transform=ax.transAxes, va='top', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.savefig('indexing_analysis_results.png', dpi=300, bbox_inches='tight')
    print("\n✓ Visualization saved as 'indexing_analysis_results.png'")
    plt.show()

def run_complete_indexing_analysis(conn):
    """Run complete indexing analysis for all 4 queries"""

    print("\n" + "="*80)
    print("PART 2: COMPREHENSIVE INDEXING ANALYSIS")
    print("="*80)
    print("\nTesting different index configurations for each advanced query...")
    print("Each query is run 3 times per configuration; median time is reported.")

    # Define the 4 advanced queries
    query1 = """
    SELECT
        p.product_id,
        p.category_name,
        COUNT(DISTINCT o.order_id) as total_orders,
        SUM(oi.quantity) as total_quantity_sold,
        SUM(oi.quantity * oi.unit_price) as total_revenue,
        AVG(oi.unit_price) as avg_selling_price,
        MAX(o.purchase_ts) as last_sold_date
    FROM Products p
    INNER JOIN Order_Items oi ON p.product_id = oi.product_id
    INNER JOIN Orders o ON oi.order_id = o.order_id
    WHERE o.status IN ('delivered', 'shipped')
    GROUP BY p.product_id, p.category_name
    HAVING total_revenue > 1000
    ORDER BY total_revenue DESC
    LIMIT 15
    """

    query2 = """
    WITH CustomerMetrics AS (
        SELECT
            c.customer_id,
            c.name,
            c.city,
            c.state,
            COUNT(DISTINCT o.order_id) as order_count,
            SUM(p.amount) as total_spent,
            MIN(o.purchase_ts) as first_purchase,
            MAX(o.purchase_ts) as last_purchase
        FROM Customers c
        INNER JOIN Orders o ON c.customer_id = o.customer_id
        INNER JOIN Payments p ON o.order_id = p.order_id
        WHERE o.status != 'canceled'
        GROUP BY c.customer_id, c.name, c.city, c.state
    ),
    ReviewMetrics AS (
        SELECT
            customer_id,
            COUNT(*) as review_count,
            AVG(score) as avg_rating
        FROM Reviews
        GROUP BY customer_id
    )
    SELECT
        cm.customer_id,
        cm.name,
        cm.city,
        cm.state,
        cm.order_count,
        ROUND(cm.total_spent, 2) as total_spent,
        COALESCE(rm.review_count, 0) as reviews_written,
        ROUND(COALESCE(rm.avg_rating, 0), 2) as avg_rating_given,
        julianday(cm.last_purchase) - julianday(cm.first_purchase) as customer_lifetime_days,
        ROUND(cm.total_spent / cm.order_count, 2) as avg_order_value
    FROM CustomerMetrics cm
    LEFT JOIN ReviewMetrics rm ON cm.customer_id = rm.customer_id
    WHERE cm.total_spent > (
        SELECT AVG(total_spent) * 0.5
        FROM CustomerMetrics
    )
    ORDER BY cm.total_spent DESC
    LIMIT 15
    """

    query3 = """
    SELECT
        p.product_id,
        p.category_name,
        i.available_qty,
        i.reserved_qty,
        COALESCE(sales_data.units_sold_30d, 0) as units_sold_30d,
        COALESCE(sales_data.units_sold_7d, 0) as units_sold_7d,
        CASE
            WHEN COALESCE(sales_data.units_sold_7d, 0) > 0
            THEN ROUND(CAST(i.available_qty AS FLOAT) / (sales_data.units_sold_7d * 4.3), 2)
            ELSE 999
        END as weeks_of_inventory,
        COALESCE(pending.pending_orders, 0) as pending_order_count,
        i.restock_date
    FROM Products p
    INNER JOIN Inventory i ON p.product_id = i.product_id
    LEFT JOIN (
        SELECT
            oi.product_id,
            SUM(CASE WHEN o.purchase_ts >= date('now', '-30 days')
                THEN oi.quantity ELSE 0 END) as units_sold_30d,
            SUM(CASE WHEN o.purchase_ts >= date('now', '-7 days')
                THEN oi.quantity ELSE 0 END) as units_sold_7d
        FROM Order_Items oi
        INNER JOIN Orders o ON oi.order_id = o.order_id
        WHERE o.status IN ('delivered', 'shipped')
        GROUP BY oi.product_id
    ) sales_data ON p.product_id = sales_data.product_id
    LEFT JOIN (
        SELECT
            oi.product_id,
            COUNT(DISTINCT o.order_id) as pending_orders
        FROM Order_Items oi
        INNER JOIN Orders o ON oi.order_id = o.order_id
        WHERE o.status = 'processing'
        GROUP BY oi.product_id
    ) pending ON p.product_id = pending.product_id
    WHERE i.available_qty < 50
        OR (sales_data.units_sold_7d > 0 AND i.available_qty / (sales_data.units_sold_7d * 4.3) < 2)
    ORDER BY weeks_of_inventory ASC
    LIMIT 15
    """

    query4 = """
    WITH PaymentSummary AS (
        SELECT
            o.order_id,
            o.status,
            o.purchase_ts,
            GROUP_CONCAT(DISTINCT p.method) as payment_methods,
            COUNT(DISTINCT p.payment_id) as payment_count,
            SUM(p.amount) as total_paid,
            MAX(p.total_installments) as max_installments
        FROM Orders o
        INNER JOIN Payments p ON o.order_id = p.order_id
        GROUP BY o.order_id, o.status, o.purchase_ts
    ),
    OrderTotals AS (
        SELECT
            o.order_id,
            SUM(oi.quantity * oi.unit_price + oi.freight_value) as order_total
        FROM Orders o
        INNER JOIN Order_Items oi ON o.order_id = oi.order_id
        GROUP BY o.order_id
    )
    SELECT
        ps.payment_methods,
        COUNT(DISTINCT ps.order_id) as order_count,
        SUM(CASE WHEN ps.status = 'delivered' THEN 1 ELSE 0 END) as delivered_count,
        SUM(CASE WHEN ps.status = 'canceled' THEN 1 ELSE 0 END) as cancelled_count,
        ROUND(AVG(ot.order_total), 2) as avg_order_value,
        ROUND(AVG(ps.payment_count), 2) as avg_payment_splits,
        ROUND(AVG(ps.max_installments), 2) as avg_installments,
        ROUND(SUM(ps.total_paid), 2) as total_revenue,
        ROUND((SUM(CASE WHEN ps.status = 'delivered' THEN 1 ELSE 0 END) * 100.0 /
         COUNT(DISTINCT ps.order_id)), 2) as delivery_rate
    FROM PaymentSummary ps
    INNER JOIN OrderTotals ot ON ps.order_id = ot.order_id
    GROUP BY ps.payment_methods
    HAVING order_count > 10
    ORDER BY total_revenue DESC
    LIMIT 15
    """

    # Define index configurations for each query
    query1_indexes = [
        ("Config 1: Status+Time Composite", [
            "CREATE INDEX idx_test_orders_status_time ON Orders(status, purchase_ts)"
        ]),
        ("Config 2: Join Columns", [
            "CREATE INDEX idx_test_order_items_joins ON Order_Items(order_id, product_id)"
        ]),
        ("Config 3: Combined", [
            "CREATE INDEX idx_test_orders_status_time ON Orders(status, purchase_ts)",
            "CREATE INDEX idx_test_order_items_product ON Order_Items(product_id)"
        ])
    ]

    query2_indexes = [
        ("Config 1: Status Only", [
            "CREATE INDEX idx_test_orders_status ON Orders(status)"
        ]),
        ("Config 2: Join Columns", [
            "CREATE INDEX idx_test_orders_customer ON Orders(customer_id)",
            "CREATE INDEX idx_test_payments_order ON Payments(order_id)"
        ]),
        ("Config 3: Full Optimization", [
            "CREATE INDEX idx_test_orders_customer_status ON Orders(customer_id, status)",
            "CREATE INDEX idx_test_payments_order ON Payments(order_id)",
            "CREATE INDEX idx_test_reviews_customer ON Reviews(customer_id)"
        ])
    ]

    query3_indexes = [
        ("Config 1: Available Qty", [
            "CREATE INDEX idx_test_inventory_qty ON Inventory(available_qty)"
        ]),
        ("Config 2: Status+Time", [
            "CREATE INDEX idx_test_orders_status_time ON Orders(status, purchase_ts)"
        ]),
        ("Config 3: Combined", [
            "CREATE INDEX idx_test_inventory_qty ON Inventory(available_qty)",
            "CREATE INDEX idx_test_orders_status_time ON Orders(status, purchase_ts)",
            "CREATE INDEX idx_test_order_items_product ON Order_Items(product_id)"
        ])
    ]

    query4_indexes = [
        ("Config 1: Timestamp", [
            "CREATE INDEX idx_test_orders_timestamp ON Orders(purchase_ts)"
        ]),
        ("Config 2: Payment Method", [
            "CREATE INDEX idx_test_payments_method ON Payments(method)"
        ]),
        ("Config 3: Status+Time+Join", [
            "CREATE INDEX idx_test_orders_status_time ON Orders(status, purchase_ts)",
            "CREATE INDEX idx_test_payments_order ON Payments(order_id)"
        ])
    ]

    # Run analysis for all queries
    all_results = []
    query_names = [
        "Top Selling Products",
        "Customer Lifetime Value",
        "Inventory Analysis",
        "Payment Method Analysis"
    ]

    queries = [query1, query2, query3, query4]
    index_configs = [query1_indexes, query2_indexes, query3_indexes, query4_indexes]

    for query, name, configs in zip(queries, query_names, index_configs):
        results = analyze_query_with_indexes(conn, query, name, configs)
        all_results.append(results)

    # Generate summary report
    print("\n" + "="*80)
    print("INDEXING ANALYSIS SUMMARY")
    print("="*80)

    for i, (query_name, results) in enumerate(zip(query_names, all_results), 1):
        print(f"\n{i}. {query_name}")
        print("-" * 70)
        baseline = results[0][1]
        print(f"   Baseline: {baseline:.2f} ms")

        best_config = min(results[1:], key=lambda x: x[1])
        improvement = ((baseline - best_config[1]) / baseline) * 100

        print(f"   Best Configuration: {best_config[0]}")
        print(f"   Best Time: {best_config[1]:.2f} ms")
        print(f"   Improvement: {improvement:.1f}%")

        print("\n   All Results:")
        for config_name, exec_time in results:
            perf_change = ((baseline - exec_time) / baseline) * 100
            print(f"     - {config_name:30s}: {exec_time:7.2f} ms ({perf_change:+6.1f}%)")

    # Create visualization
    print("\n" + "="*80)
    print("Generating performance visualization...")
    plot_indexing_results(all_results, query_names)

    print("\n✓ Complete indexing analysis finished!")
    print("✓ Take screenshots of the output above for your report")
    print("✓ Use the chart 'indexing_analysis_results.png' in your documentation")

def main():
    """Main execution function"""
    print("="*80)
    print("ULTIMATE E-COMMERCE DATABASE IMPLEMENTATION")
    print("="*80)

    # Create new database
    db_name = "ultimate_ecommerce.db"
    conn = sqlite3.connect(db_name)
    print(f"\n✓ Connected to database: {db_name}")

    # Try to load Olist data
    print("\nLoading Olist dataset...")
    olist_data = load_from_kaggle()


    if olist_data is None:
        print("ERROR: Could not load Olist dataset. Please ensure you have:")
        print("1. Installed kagglehub: pip install kagglehub")
        print("2. Or downloaded the SQLite database file manually")
        return

    # Create schema
    create_database_schema(conn)

    # Create indexes
    create_indexes(conn)

    # Populate database
    populate_database(conn, olist_data)

    # Get statistics
    get_database_statistics(conn)

    # Run advanced queries
    run_advanced_queries(conn)

    run_complete_indexing_analysis(conn)

    conn.close()
    print("\n" + "="*80)
    print("ALL TASKS COMPLETED SUCCESSFULLY!")
    print("="*80)

main()

import sqlite3
import pandas as pd

conn = sqlite3.connect("ultimate_ecommerce.db")
cursor = conn.cursor()

cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
table_names = cursor.fetchall()

for table in table_names:
    table_name = table[0]
    print(f"\nSchema for table: {table_name}")
    print("-" * (len(table_name) + 18))
    cursor.execute(f"PRAGMA table_info('{table_name}');")
    schema = cursor.fetchall()
    schema_df = pd.DataFrame(schema, columns=['cid', 'name', 'type', 'notnull', 'dflt_value', 'pk'])
    display(schema_df)

conn.close()

